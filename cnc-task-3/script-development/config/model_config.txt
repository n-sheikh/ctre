model: SyntacticTransformerWrapper
llm_name: bert-base-uncased
llm_hidden_dropout_prob: 0.1
llm_attention_probs_dropout_prob: 0.1
input_embedding_dimension: 768
dependency_embedding_dimension: 64
transformer_ffn_dimension: 1000
pos_encoder_dropout_prob: 0.1
encoder_layer_dropout_prob: 0.1
encoder_layer_hidden_dim_size: 812
nos_encoder_heads: 7
nos_encoder_layers: 6
max_nos_token: 84
pooling: attn
rnn_type: lstm
num_mech: 2
num_active: 2
hidden_size: 256
input_sizes: 768-44
classes: 2
