experiment_identifier: llm-baseline-[bert|sbert]
run_identifier: sbert-all-mpnet-base-v2-lr-[0.0005|0.00001|0.000001]
max_epochs: 3
loss_functions: cross-entropy-loss
learning_rates: 0.00005, 0.00001, 0.000005
batch_sizes: 1
optimizers: adam
llm_name: sentence-transformers/all-mpnet-base-v2
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
pooling_strategy: cls
