model: Transformer
llm_name: bert-base-uncased
llm_hidden_dropout_prob: 0.1
llm_attention_probs_dropout_prob: 0.1
pooling_strategy: cls
mr_rnn_type: lstm
mr_num_mech: 1
mr_num_active: 1
mr_hidden_size: 256
mr_input_size: 786
