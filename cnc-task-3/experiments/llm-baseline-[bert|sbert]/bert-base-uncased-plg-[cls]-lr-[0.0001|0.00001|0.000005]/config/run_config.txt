experiment_identifier: llm-baseline-[bert|sbert]
run_identifier: bert-base-uncased-plg-[cls]-lr-[0.0001|0.00001|0.000005]
max_epochs: 3
loss_functions: cross-entropy-loss
learning_rates: 0.00001, 0.00001, 0.000005
batch_sizes: 1
optimizers: adam
llm_name: bert-base-uncased
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
pooling_strategy: cls
